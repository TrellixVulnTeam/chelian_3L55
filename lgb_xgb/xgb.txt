https://www.jianshu.com/p/1100e333fcab
xgb的booster参数：
xgb的参数分为三类:通用参数，Booster参数，学习目标参数
通用参数：
    booster：我们有两种参数选择，gbtree和gblinear。gbtree是采用树的结构来运行数据，而gblinear是基于线性模型
    silent:静默模式，为1时模型运行不输出。
    nthread:使用线程数，一般我们设置成-1,使用所有线程。如果有需要，我们设置成多少就是用多少线程。
Booster参数:
    n_estimator:也作num_boosting_rounds,这是生成的最大树的数目，也是最大的迭代次数。
    learning_rate:也作eta,默认0.3每一步迭代的步长，很重要。太大了运行准确率不高，
        太小了运行速度慢。我们一般使用比默认值小一点，0.1左右就很好。
    gamma:默认0.在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。gamma指定了节点分裂所需
        的最小损失函数下降值。 这个参数的值越大，算法越保守。因为gamma值越大的时候，损失函数
        下降更多才可以分裂节点。所以树生成的时候更不容易分裂节点。范围: [0,∞]
    subsample：默认1.这个参数控制对于每棵树，随机采样的比例。减小这个参数的值，算法会更加保守，
        避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。 典型值：0.5-1，0.5代表平均采样，
        防止过拟合. 范围: (0,1]，注意不可取0
    colsample_bytree：系统默认值为1。我们一般设置成0.8左右。
        用来控制每棵随机采样的列数的占比(每一列是一个特征)。 典型值：0.5-1范围: (0,1]
    colsample_bylevel：默认为1,我们也设置为1.
        这个就相比于前一个更加细致了，它指的是每棵树每次节点分裂的时候列采样的比例
    max_depth:默认为6
        我们常用3-10之间的数字。这个值为树的最大深度。这个值是用来控制过拟合的。
        max_depth越大，模型学习的更加具体。设置为0代表没有限制，范围: [0,∞]
    max_delta_step：默认0,我们常用0.
        这个参数限制了每棵树权重改变的最大步长，如果这个参数的值为0,则意味着没有约束。
        如果他被赋予了某一个正值，则是这个算法更加保守。通常，这个参数我们不需要设置，
        但是当个类别的样本极不平衡的时候，这个参数对逻辑回归优化器是很有帮助的。
    lambda:也称reg_lambda,默认值为0。
        权重的L2正则化项。(和Ridge regression类似)。这个参数是用来控制XGBoost的正则化部分的。
        这个参数在减少过拟合上很有帮助
    alpha:也称reg_alpha默认为0,
        权重的L1正则化项。(和Lasso regression类似)。 可以应用在很高维度的情况下，使得算法的速度更快。
    scale_pos_weight:默认为1
        在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。
        通常可以将其设置为负样本的数目与正样本数目的比值
学习目标参数
    https://www.cnblogs.com/roygood/articles/10404478.html
    结论:
        binary:logistic和 'objective': 'reg:logistic'的输出是一样的,都是预测的概率
        binary:logitraw是输出的得分，用sigmoid（）函数处理后就和上述两个概率值一致
    objective [缺省值=reg:linear]
    reg:linear– 线性回归
    reg:logistic – 逻辑回归
    binary:logistic – 二分类逻辑回归，输出为概率
    binary:logitraw – 二分类逻辑回归，输出的结果为wTx
    count:poisson – 计数问题的poisson回归，输出结果为poisson分布。在poisson回归中，max_delta_step的缺省值为0.7 (used to safeguard optimization)
    multi:softmax – 设置 XGBoost 使用softmax目标函数做多分类，需要设置参数num_class（类别个数）
    multi:softprob – 如同softmax，但是输出结果为ndata*nclass的向量，其中的值是每个数据分为每个类的概率。
eval_metric [缺省值=通过目标函数选择]
    rmse: 均方根误差
    mae: 平均绝对值误差
    logloss: negative log-likelihood
    error: 二分类错误率。其值通过错误分类数目与全部分类数目比值得到。对于预测，预测值大于0.5被认为是正类，其它归为负类。 error@t: 不同的划分阈值可以通过 ‘t’进行设置
    merror: 多分类错误率，计算公式为(wrong cases)/(all cases)
    mlogloss: 多分类log损失
    auc: 曲线下的面积
    ndcg: Normalized Discounted Cumulative Gain
    map: 平均正确率

parameters = {
              'max_depth': [5, 10, 15, 20, 25],
              'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15],
              'n_estimators': [500, 1000, 2000, 3000, 5000],
              'min_child_weight': [0, 2, 5, 10, 20],
              'max_delta_step': [0, 0.2, 0.6, 1, 2],
              'subsample': [0.6, 0.7, 0.8, 0.85, 0.95],
              'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9],
              'reg_alpha': [0, 0.25, 0.5, 0.75, 1],
              'reg_lambda': [0.2, 0.4, 0.6, 0.8, 1],
              'scale_pos_weight': [0.2, 0.4, 0.6, 0.8, 1]
            }

xlf = xgb.XGBClassifier(max_depth=10,
            learning_rate=0.01,
            n_estimators=2000,
            silent=True,
            objective='binary:logistic',
            nthread=-1,
            gamma=0,
            min_child_weight=1,
            max_delta_step=0,
            subsample=0.85,
            colsample_bytree=0.7,
            colsample_bylevel=1,
            reg_alpha=0,
            reg_lambda=1,
            scale_pos_weight=1,
            seed=1440,
            missing=None)

# 有了gridsearch我们便不需要fit函数
gsearch = GridSearchCV(xlf, param_grid=parameters, scoring='accuracy', cv=3)
gsearch.fit(train_x, train_y)

print("Best score: %0.3f" % gsearch.best_score_)
print("Best parameters set:")
best_parameters = gsearch.best_estimator_.get_params()
for param_name in sorted(parameters.keys()):
    print("\t%s: %r" % (param_name, best_parameters[param_name]))


# 使用交叉验证 xgb.cv
params={
    'booster':'gbtree',
    'gamma':0.05,#树的叶子节点下一个区分的最小损失，越大算法模型越保守
    'max_depth':12,
    'lambda':450,#l2正则项权重
    'subsample':0.4,#采样训练数据，设置为0.5
    'colsample_bytree':0.7,#构建树时采样比例
    'min_child_weight':12,#节点的最少特征树
    'slient':0,#为1时模型运行不输出。
    'eta':0.005,#类似学习率
    'seed':2,
    'nthread':4
}
watchlist =[(dtrain,'train'),(dtest,'val')]
cv_results = xgb.cv(params, dtrain, num_boost_round=5, nfold=3, seed=0 ,
                maximize=False, early_stopping_rounds=2)
#cv_results.keys()#Index(['test-rmse-mean', 'test-rmse-std', 'train-rmse-mean', 'train-rmse-std'], dtype='object')
xgb_model = xgb.train(params,dtrain,num_boost_round=50,evals=watchlist)

xgb_model.save_model('...')
xgb_model.dump_model('...')
#数据集保存
dtest.save_binary('...')

xgb.Booster(model_file='')

使用pickle保存
如果使用xgb原生API，预测时字段的顺序和训练时必须保存一致，原生API没有predict_proba方法
如果使用sklearn风格的API，则不必保持一致,不必保持一致的原因很有可能是把数据当成了np.array来处理，
    每次随机列的顺序时，输出的结果不一致。而顺序不变时输出的结果是一样的
如果模型配置了early_stopping_rounds这个参数，就有有best_score, best_iteration, best_ntree_limit3个参数。
在预测的时候，如果是sklearn风格的predict，ntree_limit默认就是best_ntree_limit,而如果是xgb原生风格的predict方法，ntree_limit要自己设置
# xgboost.sklearn.py 929
if early_stopping_rounds is not None:
    self.best_score = self._Booster.best_score
    self.best_iteration = self._Booster.best_iteration
    self.best_ntree_limit = self._Booster.best_ntree_limit

model.get_booster().predict也即原生API，如果不指定ntree_limit=model.best_ntree_limit，则好像是所有的，但是数量又对不上
使用bst.trees_to_dataframe()获取树的数目
model_es.get_booster().trees_to_dataframe().Tree.nunique() # 50
该数据即为画图时显示的树的数目
model_es.best_ntree_limit 30 ,如果采用CV的方式，最佳数的数目(best_ntree_limit)和数的数目(model_es.get_booster().trees_to_dataframe().Tree.nunique())不一致
