VAE:变分自编码器。
1. 假设p(Z|X)（后验分布）是高斯分布，用神经网络(encoder)来计算均值和方差。然后从专属的正太分布中采样一个Zk
    出来，经过一个生成器得到Xk=g(Zk)。现在可以放心的最小化(Xk,Xk)^2了。
    我们希望重构 X，也就是最小化 D(X̂k,Xk)^2，但是这个重构过程受到噪声的影响，
    因为Zk 是通过重新采样过的，不是直接由 encoder 算出来的。显然噪声会增加重构的难度，
    不过好在这个噪声强度（也就是方差）通过一个神经网络算出来的，所以最终模型为了重构得更好，肯定会想尽办法让方差为0。
    说白了，模型会慢慢退化成普通的 AutoEncoder，噪声不再起作用。 VAE 要求每个 p(Z_X) 都向正态分布看齐。
    将正太分布和标准正太分布的KL散度作为额外的loss。
    VAE希望后验分布是正太分布，然后从正太分布中采样出z送给解码器解出X，重构误差。但如果仅仅是这样的话，模型会慢慢的
    退化成普通的AutoEncoder,噪声不再起作用。所以VAE要求后验分布向标准正太分布看齐。将KL散度加入到模型中，起到
    正则的作用。
2. 一个好的潜变量模型就在于，它的变换f(z,θ)能将潜变量的分布变换为观测数据X的分布。
    换句话说，它能使观测数据X的似然函数P(X)最大。如果我们能通过优化使得P(X)最大，
    那么就能得到这个潜变量模型了。这是极大似然法的思想。要想优化P(X)，我们首先需要得到它关于z和θ的表达式：
    可是基于文中提到的2个原因，直接优化P(X)很难。
    z能变化为X，那么相应的某一个X也能变为对应的z。既然我们想要使得Q(z)分布中的z被变换为我们所想要的x的可能性比较大，
    很明显如果这个Q分布就是 [公式] 的话，那再好不过了，因为这时候每个z一定能被变换到x
3. 如果X能变换位z的话，那么就存在逆变换，将z变换为X，可是求P(z|x)的贝叶斯公式中的证据分布P(X)在高维空间中很难求。
    因此，我们用神经网络去拟合这个分布。

VAE和GAN一样，都是从标准正太分布中采样，训练一个生成模型，将原来的概率分布映射到训练集的概率分布。都是进行分布之间的变换
怎么衡量X和g(z)的相似呢？WGan直接用神经网络来拟合。而VAE采用重构误差
终极生成模型，有训练数据直接得到P(X)，然后根据P(X)采样
退而求其次的生成模型:从标准正太分布中采样一个Z，然后根据Z来计算一个X，也是一个很棒的生成模型。

在VAE中，其实可以根据一系列的X来得到P(X)，采用mle的思想，用MCGE(蒙特卡洛梯度估计)来求解theta，关键是P(X)不好采样。
VAE是个生成模型，如果能到得P(Z|X)的话，从x采用一个z，再变成x最好了，所有主要计算p(z|x)，由于这很难计算，故用神经
    网络来拟合